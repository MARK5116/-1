# 常见问题

1. xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？

疑问：

用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了。请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？

答案：

Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。

就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X^2)-[E(X)]^2）。

偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；

方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。

当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。 

当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。

所以，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。

对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。

对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。

# Xgboost

XGB模型，是一个可以用于回归和分类任务的模型，其输入是n组带有label的数据，输出是回归结果或者分类结果。

## XGB模型概述

1. XGB：多个弱分类器集成强分类器

XGB，全名eXtreme Gradient Boosting，XGB也是Boosting的一种，由多个弱分类器，集成在一起，形成一个强分类器。

2. 构成XGB模型的弱分类器就是常见的决策树模型

3. 常用参数

那强分类器是由多少个弱分类器组成的呢？这是由参数迭代次数n_estimator 决定的，迭代多少次，就生成多少个弱分类器。

每棵决策树分裂到多深才能停止呢？这是由参数最大深度max_depth决定的，max_depth值是多少，树就分裂至多深。

## XGB模型的形成过程

1. 每个弱分类器生成时，拟合的是上一个分类器预测值的残差

XGB中每一个弱分类器的生成过程就是拟合上一个分类器预测值的残差。

比如，label值为1，第一个弱分类器输出的预测值为0.2，则第二个弱分类器的拟合目标不再是1，而是label值和第一个弱分类器的残差0.8。

因此，当我们训练完成得到K棵树的时候，我们需要预测的样本，在每棵树上都会对应落到一个叶子节点，每个叶子节点会对应一个分数，也就是当前弱分类器对当前样本的预测值，最后将每棵树对应的分数加起来，就是最终的预测值。

如图，假设我们训练得到一个由两个弱分类器组成的XGB模型，则样本I3的最终预测值为0.1+0.14=0.24。

2. 遍历所有特征的取值，在增益最大处分裂

每个训练样本都是由n个特征组成，每个特征的维度不尽相同，树的形成过程实际就是叶子节点不断生成的过程，即当前根节点该从哪个特征的哪个取值分裂？如上图，选择从特征f2的取值为5处分裂。

特征及其取值的选取方式：遍历所有特征的所有特征点，计算其增益，在增益最大的特征取值处分裂。此模型使用目标函数来计算其增益。

3. XGB模型的目标函数

XGB的目标函数=损失函数+正则项

**损失函数：**XGB模型的损失函数可以任意定义，只要二阶可导即可（后续解释原因）

**正则项：**其中T是叶子节点的数目，w是每个叶子节点的分数。通过正则项的设定来控制叶子节点的数目及叶子节点的分数，从而来降低模型复杂度，减小训练方差，防止过拟合。

## XGB模型

1. 模型特点：速度快，效果好；

xgb全称eXtreme Gradient Boosting。xgb模型是树模型的一种，从其名字可以看出，主要做了以下几个方面的改进：

Gradient：xgb和GBDT模型一样，主要是用到了残差。第n棵树主要拟合了前n-1棵树和预测值之间的残差；

Boosting：Boosting就是将多个弱分类器，集成一个强分类器的过程。最终的结果是所有分类器结果之和。

Extreme：在原GBDT模型的基础上，又做了很多新的改进。

2. XGB的输入和输出

输入是训练集样本I={(x,y1),(x2,y2),...(xm,ym)}I={(x,y1),(x2,y2),...(xm,ym)}

输出是强学习器y=f(x)

3. XGB的特点

- 并行分裂，并对训练的每个特征进行排序分块存储
- 添加正则项
- 自动化处理缺失值。有缺失值的情况，样本去左右子树会计算两次，全部去左子树或者全部去右子树。
  当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。
- 剪枝：会一直分裂到一个最大深度，再返回来进行剪枝。
- 交叉验证。

4. XGB模型的目标函数

目标函数=损失函数+正则项

xgb的正则项是关于树的数量T和叶子节点的权重w的函数，通过正则项的设置，可以有效控制树的数量及叶子节点的权重值，通过减少权重的值，可以减小模型的方差，使其更容易拟合，缓解过拟合的情况。

损失函数有很多种：

linear：默认线性回归

logistic：二分类的逻辑回归

softmax：多分类器

5. XGB的优化过程

XGB优化损失函数的过程就是，每次分裂都期望最大程度降低损失函数。经过各种推导之后，变成最大化一个关于一阶导和二阶导的式子score。

每次分裂都要计算所有特征的所有分裂节点处的score值来确定分裂方法。

如果当前score为0，则建树完毕，计算所有叶子区域的wtj, 得到弱学习器ht(x)，更新强学习器ft(x)。

损失函数，优化方法，求解过程，主要参数

6. XGB参数

通用参数：

- booster，有gbtree和gbline两种，常用gbtree（xgb可以用线性模型，但是常用tree模型，线性模型很少用到，因为效果不够好）
- silent，默认0；当该参数为1时，静默模式开启，不会输出任何消息
- nthread，线程数目。自动检测最大线程数，默认值是最大值。

Booster参数：

- eta，默认0.3；和lr类似，可以减少每步的权重，可以提升模型的鲁棒性。
- min_child_weight，默认1；最小叶子权重之和（防止过拟合）
- max_depth，默认6；树的最大深度（此参数过大容易造成过拟合）
- max_leaf_nodes，树的最大叶子数量（二叉树时可由max_depth代替）
- gamma，默认0；指定节点分裂时所需的最小损失函数下降值
- max_delta_step，默认0；限制每棵树权重改变的最大步长
- subsample，默认1；控制每棵树随机采样的比例（防止过拟合）
- colsample_bytree，默认1；每棵树随机采样列数占比
- colsample_bylecel，默认1；控制每级每次分裂对列数采样占比
- lambda，默认1；L2正则化项参数
- alpha，默认1；L1正则化项参数
- 迭代次数：树的数量

学习目标参数：

- 多个可选择的目标函数

7. 评价指标

分类问题：error（二分类）,  merror（多分类）

回归问题：rmse, mae

8. xgb模型特征重要程度的计算

在决策树的过程中，通过每个特征分列的点改进性能的量来计算属性的重要性，由节点负责加权和记录次数。一个特征对分裂点改进性能度量越大，即越靠近根节点，则权值越大；被越多的提升树所选择，属性就越重要。

9. xgb的权重衰减

xgb在迭代完成后，会在叶子节点上乘一个衰减系数，主要是为了削弱每棵树的影响。通常，Boosting模型考虑的是**弱**分类器的组合，因此，我们可以减小衰减系数eta，增大树的数量。

10. xgb模型的并行结果

xgb模型速度快的原因：

- 对特征值进行排序，保存为block结构，后面的实验反复调用这个结构；
- 在进行节点分裂时，要计算每个特征分裂点的增益，计算增益时，使用并行运算；
- 没有使用贪心算法，而是使用可并行的近似直方图算法



11. 回归问题的损失函数

<https://www.jiqizhixin.com/articles/2018-06-21-3>五大回归模型的损失函数；

12. xgb模型为什么要泰勒展开

主要目的应该是 可以定义任意目标函数，只要有二阶导就可以。（二阶比一阶下降更快？精度更高？使用泰勒展开可以自定义目标函数？）

13. xgb模型什么时候停止分裂

当引入的分裂低于一定的阈值的时候，会停止分裂；且当树分裂到最大的深度时，开始剪枝，剪掉gini没有上升的枝。

14. xgb在进行节点分裂时，为了提高效率，没有使用完全的贪心算法，而是使用了可并行的近似直方图算法，跟LightGBM中采用的算法类似。

LightGBM采用Histogram算法，其思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram。然后遍历训练数据，统计每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点。

## 目标函数

目标函数由两部分组成：训练的损失（预测值与真实值的差异）和正则化。

相比GBDT，1）它的损失函数是支持自定义的2）在目标函数中加入了正则化项，用于控制模型的复杂度，防止过拟合，从而提高模型的泛化能力。

### 正则化项

树的复杂度由两部分组成：叶子结点个数和叶结点分数。

叶结点个数可以衡量树的复杂度，叶结点越多，树越复杂，过拟合风险越高。树的个数越多，惩罚力度越大，每个叶子结点上都有一个权重值，不论是线性回归还是神经网络，l2惩罚项，而对于叶结点得分做了l2正则，限制权值，避免叶结点分数过高。

相关问题：

- L2正则的思想是什么？优点是什么？适合什么场景？

- 为什么对叶子结点的得分做L2正则？

- 为什么要让叶子结点的得分相对较小？

  防止过拟合

注意：

- 训练损失 L(f):函数拟合坐标点的程度 ----测试样本集表现，代表经验误差 

- 正则化项Ω(f):函数自身的复杂程度 ----真实样本集表现，代表泛化能力 

- 对于一个机器学习的模型的通用原则是：简单并且准确。模型往往需要在简单和准确之中做一个折中

- 有哪些指标可以衡量树的复杂度？ 树的深度，内部节点个数，

- Xgboost 采用的 叶子节点个数(T)，叶节点分数(w)...

- 对叶子节点个数进行惩罚，相当于在训练过程中做了剪枝

### 求解方法（Gradient Boosting）

如何优化目标函数求得最小值？使用boosting思想。

最开始，对于某一个样本，一颗树都没有，在每一轮不断地加一颗树模型。可以看出，第t次迭代后，模型的预测等于前t-1次的表达效果+第t棵树的表达效果。然后目标函数做了这个转换 ，当然了这棵树不是随便加的，就是要使整体目标函数最小。

### 求解方法的优化

因为目标函数是由多个含有不同参数的弱分类器组成,无法用传统的梯度下降的形式进行直接的优化, 可以看出，第t次迭代后，模型的预测等于前t-1次的模型预测加上第t棵树的预测，那么怎么选择新加入的模型呢？肯定是使整体目标函数最小的的模型？

目标函数的求解方法。这一坨式子看起来比较恶心。第一步就是对于这个目标函数组系列一个泰勒展开来近似原来的目标函数；x相当于原来的模型，detax 相当于新加入的模型。第一步的变化就是对目标函数进行二阶泰勒的展开。第二步由于前面预测出来的已经是个固定值了，那么在加新模型的时候，这一项其实就是个常数值了，因为常数项对于优化求解其实没有什么影响的，所以之后就先约掉了。最后把欧米茄展开，来进行一些化简的操作，进行变化之后，把公式中的f就代表弱学习器，树的参数是样本的得分，接着转换一下视角，因为样本最终也是落到了叶子结点上的，所以可以统一得到。

常见问题：

1. 为啥要用二阶泰勒展开？

- 模块化：可以很方便地自定义损失函数，只要这个损失函数可以求一阶和二阶导

- 快速收敛：二阶信息本身就能让梯度收敛更快更准确

意义：

理论收益: 了解学习的本质， 快速收敛
工程收益: gi 和hi由损失函数计算得来，为已知变量
 	学习过程只和目标函数中gi 和hi有关
 	框架和策略解藕，损失函数可订制化(平方损失，逻辑损失）

## 算法优化

接下来讲树的生长过程。现在只要知道树的结构，就能得到一个该结构下的最好分数。那么问题是怎么确定树的结构呢，也就是说，怎么知道这个树是咋长的呢。

### 树的学习策略

现在只要知道树的结构，就能得到一个该结构下的最好分数

1. 怎么确定树的结构？

- 暴力法

  枚举所有可能的树结构，使用分数共识来计算树的结构分，挑个最小的，找到最好的树结构，计算叶子权重。但是对于实际问题来讲，这样显然太复杂了

- 贪心法：

  每次尝试分裂一个叶节点，计算分裂前后的增益，选择增益最大的

2. 分裂前后的增益怎么计算

- ID3：信息增益
- C4.5：信息增益比 
- CART：Gini系数 &均方误差

​      XGBoost呢？

2. 增益

分裂前后增益的定义：公式



Gain：这个分裂增益的计算方法，其实也在体现着剪枝，越大越好。

观察这个目标函数，大家会发现第二个值得注意的事情就是引入分割不一定会使得情况变好，因为我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。大家可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为 **heuristic**（启发式）而进行的操作了

伽马的作用：

一种新的分裂增益函数方法，这里其实也在体现剪枝，如果gain小于伽马，那么就不会增加分支。也就是说，伽马的大小可以控制剪枝的力度。

标红部分衡量了每个叶子节点对总体损失的的贡献，我们希望损失越小越好，则标红部分的值越大越好。

这里其实体现了剪枝，如果分数小于γ，则不会增加分支，就是伽马的大小可以控制剪枝的力度,在模型simplicity和prefictiveness中做trade-off

Pre-停止
如果增益为负就停止生长(忽略全局最优) 

lPost-剪枝
先让树长到最大深度，回头递归剪掉负增益叶子(计算复杂) 

### 树节点的分裂方法

- 精确算法

  遍历所有特征的所有可能的分割点，计算gain值，选取值最大的  (feature，value) 去分割

  缺点：当数据量大时，时间复杂度很大，精确算法就变得低效。

  对于每个节点都枚举所有的特征，然后对于每个特征，根据特征值对样本进行排序，在这个特征上，使用线性扫描决定最佳分裂点。

- 近似算法

  对于每个特征，只考察分位点，减少计算复杂度。

该算法会首先根据特征分布的百分位数(percentiles of feature distribution)，提出候选划分点(candidate splitting points)。接着，该算法将连续型特征映射到由这些候选点划分的分桶(buckets)中，聚合统计信息，基于该聚合统计找到在建议（proposal）间的最优解。

近似算法：有两种选择，一种是全局近似，也就是一开始就选好，每次划分时候都不变。另一种是每次划分后都要在分出来的样本里重新再划分。

Global：树构建的初始阶段选出所有候选候选点

Loal：每次节点分裂后重新选出候选点

global和local的思想、特点：

同等精度下，global需要的切分点多而Local需要的切分点少

分位点：近似方法通过特征的分布，按照百分比确定一组候选分裂点，通过遍历所有的候选分裂点来找到最佳分裂点

Global和local的区别：

global是在树构建的初始阶段选出所有候选分隔点，后面每层都使用相同的策略选择分隔点；而local在每次split后重新选出候选分隔点，适合较深的树，因为如果是global的话得一开始就分得超级超级细才能满足深树的需求。关于全局近似和局部近似其实是在effect和accuracy中做trade off

既然样本数量太大，我们可不可以按比例来选择，从n个样本中抽取k个样本来进行计算，取k个样本中的最优值作为split value，这样就大大减少了运算数量。这就是k分位点选取的思想，即quantile sketch。


分位点：基于特征大小排序，然后根据特征值划分（均分）

### 缺失值处理

- 传统做法：

​       删除 （丢失部分特征信息）

​       填补 （会改变样本的特征分布）

- XGBoost：

  对于缺失值，可以学习出默认的节点分裂方向

  最佳缺省方向确定：

  ​	只访问非缺失数据，计算复杂度与非缺失数据数目线性相关

  ​	减少时间开销，提升算法效率

描述：

当缺失值处理时，会被分到默认方向。对于默认方向的学习，在节点分裂时并不考虑缺失值的数值，缺失值数据会被分别分到左右子树中计算损失，选择较优者。

在分割的时候，系统能感知稀疏值，每个树的节点都对应一个默认方向，当缺失值出现时，会被分类到默认方向。

对于最优缺失方向的查找，论文中关于缺失值的处理将其看与稀疏矩阵的处理看作一样。在寻找**split point**的时候，不会对该特征为**missing**的样本进行遍历统计，只对该列特征值为**non-missing**的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找**split point**的时间开销，提升算法的效率，**paper** 提到能提高**50**倍。

在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。

### 列抽样(column sample)

借鉴了随机森林的做法，构建每棵树时对属性进行采样，不仅能降低过拟合风险，还能减少计算

### Shrinkage（缩减）即学习速率

在梯度提升的每一步之后，增加权重，类似于随机优化过程中的学习率, 减少单颗树的影响

### 支持自定义损失函数

只要支持二阶可导

### 防止过拟合方法：

除了正则化的方法，还有两种方式。第一种技术是**Shrinkage**。Shrinkage会在每一步tree boosting时，将新加入的**weights**通过一个因子**η**进行缩放。与随机优化中的**learning rate**相类似，对于用于提升模型的新增树(future trees)，**shrinkage**可以减少每棵单独的树、以及叶子空间（leaves space）的影响。第二个技术是**列特征子抽样**(column feature subsampling)。该技术也会在RandomForest中使用，使用列子抽样可以阻overfitting。列子抽样的使用可以加速并行算法的计算。

在梯度提升的每一步之后，增加权重，类似于随机优化过程中的**学习率，**减少单棵树的影响。

## 总结

1. 快&可扩展性

•自定义损失函数

•Split finding 算法优化

​     -近似分位点

​     -缺失值处理

•工程实现

​      -并行计算

​      -内存缓存

2. 准：解决过拟合问题

•控制模型复杂度

​      \- 目标函数正则化项

​      \- 建树与剪枝

•增加随机性

​      -column subsampling

​     \- shrinkage

首先过拟合 在机器学习中比较常见，这是因为机器学习算法为了满足尽可能复杂的任务，其模型的拟合能力一般远远高于问题复杂度，也就是说，机器学习算法有「拟合出正确规则的前提下，进一步拟合噪声」的能力。
 