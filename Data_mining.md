# 特征工程

**数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。**

特征工程的目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面：

![avatar](/Users/didi/Desktop/杨记好/简历/5154e035410e7f4a653d62d9608850af.jpg)

特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。

# 数据预处理

通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：

- 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。
- 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。
- 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。
- 存在缺失值：缺失值需要补充。
- 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。

## 无量纲化

在进行特征选择之前，一般会先进行数据无量纲化处理，这样，表征不同属性（单位不同）的各特征之间才有可比性，如1cm 与 0.1kg 你怎么比？

在涉及到计算点与点之间的距离时，使用归一化或标准化都会对最后的结果有所提升，甚至会有质的区别。

**归一化与标准化之间应该如何选择呢？**

- 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准差表现更好。
- 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。
- 如果把所有维度的变量一视同仁，在最后计算距离中发挥相同的作用应该选择标准化，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择归一化。另外，标准化更适合现代嘈杂大数据场景。

### 标准化

常用的方法是z-score标准化，经过处理后的数据均值为0，标准差为1，处理方法是：

$$x'=\frac{x-\mu}{\sigma}​$$

其中μ是样本的均值，σ是样本的标准差，它们可以通过现有样本进行估计。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。

回顾下正态分布的基本性质，若x～N(u,σ^2),则有

$$y=\frac{x-\mu}{\sigma}$$~N(0,1)

其中，N(0,1)表示标准正态分布。

可以看出，z-score标准化方法试图将原始数据集标准化成均值为0，方差为1且接近于标准正态分布的数据集。然而，一旦原始数据的分布不接近于一般正态分布，则标准化的效果会不好。该方法比较适合数据量大的场景(即样本足够多，现在都流行大数据，因此可以比较放心地用)。

此外，相对于min-max归一化方法，该方法不仅能够去除量纲，还能够把所有维度的变量一视同仁(因为每个维度都服从均值为0、方差1的正态分布)，在最后计算距离时各个维度数据发挥了相同的作用，避免了不同量纲的选取对距离计算产生的巨大影响。所以，涉及到计算点与点之间的距离，如利用距离度量来计算相似度、PCA、LDA，聚类分析等，并且数据量大(近似正态分布)，可考虑该方法。相反地，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择min-max归一化。

### 归一化

该方法是对原始数据进行线性变换，将其映射到[0,1]之间,该方法也被称为离差标准化。​ 

公式：$$x'=\frac{x-min}{max-min}​$$

其中，min是样本的最小值，max是样本的最大值。由于最大值与最小值可能是动态变化的，同时也非常容易受噪声(异常点、离群点)影响，因此一般适合小数据的场景。

该方法还有两点好处：

1)  如果某属性/特征的方差很小，如身高：np.array([[1.70],[1.71],[1.72],[1.70],[1.73]])，实际5条数据在身高这个特征上是有差异的，但是却很微弱，这样不利于模型的学习，进行min-max归一化后为：array([[ 0. ], [ 0.33333333], [ 0.66666667], [ 0. ], [ 1. ]])，相当于放大了差异；

2)  维持稀疏矩阵中为0的条目。

```python
def autoNorm(self,dataSet):
    #获得数据的最小/大值
    minVals = dataSet.min(0)		                 
    maxVals = dataSet.max(0)
    #最大值和最小值的范围
    ranges = maxVals - minVals	
    #返回与dataSet相同行列数的0矩阵
    normDataSet = np.zeros(np.shape(dataSet))
    #返回dataSet的行数
    m = dataSet.shape[0]
    #原始值减去最小值
    normDataSet = dataSet - np.tile(minVals, (m, 1))   
    #归一化公式
    normDataSet = normDataSet / np.tile(ranges, (m, 1))	

    return normDataSet, ranges, minVals	

```

### 区间缩放法

区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：

$$x'=\frac{x-min}{max-min}$$

### 标准化与归一化的区别

标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。

归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。

- 

## 对定量特征的二值化

## 对定性特征的编码

## 缺失值的计算

具体的常用方法如下：

1. 删除缺失值（缺失值占比很小的情况）
2. 人工填充 （数据集小，缺失值少）
3. 用全局变量填充（将缺失值填充一常数如“null”）
4. 使用样本数据的均值或中位数填充
5. 用插值法（如拉格朗日法、牛顿法）

## 数据变换



