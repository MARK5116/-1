# 数据集

**训练集：** 用于训练参数的数据子集称为训练集

**验证集：** 用于挑选超参数的数据集称为验证集

**测试集：** 用来评估算法性能的数据集称为测试集

# 交叉验证

交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　

**那么什么时候才需要交叉验证呢？**

交叉验证用在数据不是很充足的时候。比如在我日常项目里面，对于普通适中问题，如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型。如果样本大于一万条的话，我们一般随机的把数据分成三份，一份为训练集（Training Set），一份为验证集（Validation Set），最后一份为测试集（Test Set）。用训练集来训练模型，用验证集来评估模型预测的好坏和选择模型及其对应的参数。把最终得到的模型再用于测试集，最终决定使用哪个模型以及对应参数。

## 常用方法

### k折交叉验证

**基本思想：** 交叉验证法是将数据集分层K个大小相似的互斥子集（采用分层采样），每次用K-1个子集的并集作为训练集，剩下的子集作为测试集，进行K组实验，返回K个测试结果的均值。



# 过拟合 欠拟合

对于机器学习算法来说，决定算法效果的因素主要有两点：降低训练误差，缩小训练误差与测试误差的差距。对于这两个因素调整不当就会出现过拟合和欠拟合的问题。

**过拟合:** 指学习中所选模型包含的参数过多，将一些噪音等无关信息作为特征进行了学习，导致对已知数据学习过好，而对未知数据的预测能力过差。泛化能力不强，也就是说训练误差和和测试误差之间的差距太大。 

**欠拟合：** 指模型不能在训练集上获得足够低的误差。



**复杂度、过拟合、训练误差、测试误差关系：**

当模型的复杂度过大时，训练误差就会减少并趋向0，而测试误差也会先减少，达到最小值后就会增大，当选择模型复杂度过大时，就会出现过拟合问题。

**解决方法：**

通过调整模型的容量来控制模型是否偏于过拟合或者欠拟合状态。

对于过拟合问题，主要是在模型训练过程中将一些噪音信息作为特征过于拟合训练样本，导致数据预测的准确性降低，降低模型的容量可以很好的控制过拟合问题，也就是说减少输入特征的个数或者获得更多的输入样本，然而减少特征就会丢失一些信息，故通常使用正则化的方法来处理过拟合问题。

正则化的方法保留了所有的特征，减少特征的权重**θ**的值，确保了所有的特征对预测值都有少量的贡献，这样就可以解决特征过多的过拟合问题。

# 正则化

**链接：** [**https://blog.csdn.net/zouxy09/article/details/24971995**](https://blog.csdn.net/zouxy09/article/details/24971995)

​       　[**https://blog.csdn.net/jinping_shi/article/details/52433975**](https://blog.csdn.net/jinping_shi/article/details/52433975)

**正则化** 是结构风险最小化策略的实现，是在经验风险上加一个正则化项或者罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂正则化项就越大。

**引入：** 过拟合是指在模型训练过程中，将一些噪音信息作为特征过于拟合训练样本，导致数据预测准确性降低。解决过拟合就是得减少输入特征的个数，或者获得更多的输入样本，然而减少特征就会丢失一些信息，其中正则化就是一种很好的解决过拟合的方法。

**特性：** 正则化的方法保留了所有的特征，减少特征的权重θ的值，确保了所有的特征对预测值都有少量的贡献，这样就可以解决特征过多的过拟合问题。    

## L1正则L2正则区别

### L1正则化

L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。

**为什么要生成一个稀疏矩阵？**

稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。

### L2正则化

L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。

**为什么可以解决过拟合？**

拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

# 超参数

超参数不是算法训练的参数，是算法训练前给出的参数，也就是说是认为设定的参数。

**为什么设置超参数？**

对于一些参数并不适合在训练集上学习，如果在训练集上学习超参数，这些参数的结果总是趋向于最大可能的模型容量，进而导致过拟合。

例如，相比低次多项式和正的权重衰减设定，更高次的多项式和权重衰减参数设定 λ = 0 总能在训练集上更好地拟合。